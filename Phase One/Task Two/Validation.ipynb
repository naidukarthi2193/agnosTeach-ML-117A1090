{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from flask import Blueprint,jsonify,request\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "import pickle\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "def vectorize_data(data, vocab: dict) -> list:\n",
    "    keys = list(vocab.keys())\n",
    "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
    "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
    "    vectorized = list(map(encode, data))\n",
    "    return vectorized\n",
    "\n",
    "def preprocessing(input_review):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    data = input_review\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "    data_words = list(sent_to_words(data))\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5) \n",
    "    trigram = gensim.models.Phrases(bigram[data_words])  \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    data_words_nostops = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_words]\n",
    "    data_words_bigrams = [bigram_mod[doc] for doc in data_words_nostops]\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    embedding_vector_size = 256\n",
    "    bigrams_model =  Word2Vec.load(\"Bigram_Word2VecModel2.model\")\n",
    "    X_data = bigram_mod[data_lemmatized]\n",
    "    input_length = 300\n",
    "    X_pad = pad_sequences(sequences=vectorize_data(X_data, vocab=bigrams_model.wv.vocab),maxlen=input_length,padding='post')\n",
    "    return X_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vald = pd.read_csv('validation.csv')\n",
    "datas = vald[\"reviews.text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Loaded model from disk\n",
      "<keras.engine.sequential.Sequential object at 0x0000014DB086F470>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "json_file = open('Consumermodel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(\"Consumermodel.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model)\n",
    "\n",
    "pred = model.predict(preprocessing(datas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in range(0,1,0.01)\n",
    "Y_pred=list()\n",
    "for x in pred:\n",
    "    if x < 0.1:\n",
    "        Y_pred.append(0)\n",
    "    else:\n",
    "        Y_pred.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = list()\n",
    "for x in vald[\"reviews.rating\"]:\n",
    "    if x <5:\n",
    "        Y_true.append(0)\n",
    "    else:\n",
    "        Y_true.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72305"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_true, Y_pred ,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = list()\n",
    "for z in np.arange(0,1,0.001):\n",
    "    Y_pred=list()\n",
    "    for x in pred:\n",
    "        if x < z:\n",
    "            Y_pred.append(0)\n",
    "        else:\n",
    "            Y_pred.append(1)\n",
    "    accuracy.append(accuracy_score(Y_true, Y_pred ,normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.71325,\n",
       " 0.7131,\n",
       " 0.713,\n",
       " 0.71285,\n",
       " 0.71245,\n",
       " 0.712,\n",
       " 0.712,\n",
       " 0.71205,\n",
       " 0.71185,\n",
       " 0.71175,\n",
       " 0.7117,\n",
       " 0.71165,\n",
       " 0.71185,\n",
       " 0.71185,\n",
       " 0.7117,\n",
       " 0.71185,\n",
       " 0.7118,\n",
       " 0.7115,\n",
       " 0.7114,\n",
       " 0.71135,\n",
       " 0.71135,\n",
       " 0.7114,\n",
       " 0.71135,\n",
       " 0.71125,\n",
       " 0.7111,\n",
       " 0.71105,\n",
       " 0.71115,\n",
       " 0.7111,\n",
       " 0.71085,\n",
       " 0.71075,\n",
       " 0.71055,\n",
       " 0.71025,\n",
       " 0.71025,\n",
       " 0.71005,\n",
       " 0.71,\n",
       " 0.7099,\n",
       " 0.70985,\n",
       " 0.7099,\n",
       " 0.70985,\n",
       " 0.7098,\n",
       " 0.7097,\n",
       " 0.70965,\n",
       " 0.70975,\n",
       " 0.7099,\n",
       " 0.70985,\n",
       " 0.70955,\n",
       " 0.7096,\n",
       " 0.7095,\n",
       " 0.70955,\n",
       " 0.70975,\n",
       " 0.70975,\n",
       " 0.70965,\n",
       " 0.7097,\n",
       " 0.70955,\n",
       " 0.7095,\n",
       " 0.7093,\n",
       " 0.70915,\n",
       " 0.70905,\n",
       " 0.709,\n",
       " 0.70895,\n",
       " 0.709,\n",
       " 0.70875,\n",
       " 0.70875,\n",
       " 0.70865,\n",
       " 0.7085,\n",
       " 0.70835,\n",
       " 0.70805,\n",
       " 0.70785,\n",
       " 0.7078,\n",
       " 0.7076,\n",
       " 0.70755,\n",
       " 0.70745,\n",
       " 0.7075,\n",
       " 0.7074,\n",
       " 0.70735,\n",
       " 0.70735,\n",
       " 0.70715,\n",
       " 0.70695,\n",
       " 0.70695,\n",
       " 0.7069,\n",
       " 0.70665,\n",
       " 0.7066,\n",
       " 0.70655,\n",
       " 0.70635,\n",
       " 0.7063,\n",
       " 0.7063,\n",
       " 0.70605,\n",
       " 0.7061,\n",
       " 0.70615,\n",
       " 0.706,\n",
       " 0.706,\n",
       " 0.70605,\n",
       " 0.70615,\n",
       " 0.70625,\n",
       " 0.7061,\n",
       " 0.7061,\n",
       " 0.70605,\n",
       " 0.70605,\n",
       " 0.706,\n",
       " 0.7059,\n",
       " 0.70585,\n",
       " 0.70565,\n",
       " 0.70565,\n",
       " 0.70585,\n",
       " 0.70575,\n",
       " 0.70575,\n",
       " 0.7057,\n",
       " 0.70555,\n",
       " 0.70545,\n",
       " 0.70535,\n",
       " 0.70535,\n",
       " 0.70525,\n",
       " 0.70525,\n",
       " 0.70525,\n",
       " 0.7053,\n",
       " 0.7051,\n",
       " 0.70515,\n",
       " 0.70525,\n",
       " 0.70505,\n",
       " 0.7051,\n",
       " 0.7051,\n",
       " 0.70525,\n",
       " 0.70525,\n",
       " 0.7051,\n",
       " 0.7049,\n",
       " 0.7049,\n",
       " 0.7051,\n",
       " 0.70495,\n",
       " 0.7049,\n",
       " 0.70485,\n",
       " 0.70495,\n",
       " 0.70485,\n",
       " 0.70475,\n",
       " 0.7047,\n",
       " 0.70475,\n",
       " 0.70465,\n",
       " 0.7043,\n",
       " 0.70425,\n",
       " 0.70405,\n",
       " 0.704,\n",
       " 0.70375,\n",
       " 0.7038,\n",
       " 0.704,\n",
       " 0.70365,\n",
       " 0.70365,\n",
       " 0.70355,\n",
       " 0.70335,\n",
       " 0.70315,\n",
       " 0.7032,\n",
       " 0.7031,\n",
       " 0.70305,\n",
       " 0.7029,\n",
       " 0.7027,\n",
       " 0.7027,\n",
       " 0.7026,\n",
       " 0.7026,\n",
       " 0.7025,\n",
       " 0.70235,\n",
       " 0.70215,\n",
       " 0.70205,\n",
       " 0.70185,\n",
       " 0.7017,\n",
       " 0.7015,\n",
       " 0.70135,\n",
       " 0.70135,\n",
       " 0.70135,\n",
       " 0.70165,\n",
       " 0.7015,\n",
       " 0.7014,\n",
       " 0.7015,\n",
       " 0.7014,\n",
       " 0.7013,\n",
       " 0.70105,\n",
       " 0.70075,\n",
       " 0.7006,\n",
       " 0.7005,\n",
       " 0.7003,\n",
       " 0.7002,\n",
       " 0.70015,\n",
       " 0.7001,\n",
       " 0.7,\n",
       " 0.69995,\n",
       " 0.69995,\n",
       " 0.69985,\n",
       " 0.69985,\n",
       " 0.69995,\n",
       " 0.6999,\n",
       " 0.6997,\n",
       " 0.6997,\n",
       " 0.69975,\n",
       " 0.69985,\n",
       " 0.69985,\n",
       " 0.69985,\n",
       " 0.69985,\n",
       " 0.69985,\n",
       " 0.6998,\n",
       " 0.69975,\n",
       " 0.69975,\n",
       " 0.6997,\n",
       " 0.69965,\n",
       " 0.6997,\n",
       " 0.6995,\n",
       " 0.69935,\n",
       " 0.69935,\n",
       " 0.69925,\n",
       " 0.69935,\n",
       " 0.69905,\n",
       " 0.699,\n",
       " 0.6988,\n",
       " 0.69895,\n",
       " 0.6988,\n",
       " 0.6988,\n",
       " 0.69865,\n",
       " 0.69855,\n",
       " 0.6983,\n",
       " 0.6981,\n",
       " 0.69795,\n",
       " 0.6978,\n",
       " 0.69765,\n",
       " 0.6977,\n",
       " 0.6977,\n",
       " 0.69765,\n",
       " 0.69775,\n",
       " 0.6978,\n",
       " 0.6975,\n",
       " 0.6973,\n",
       " 0.6971,\n",
       " 0.69695,\n",
       " 0.6969,\n",
       " 0.69685,\n",
       " 0.69685,\n",
       " 0.697,\n",
       " 0.6969,\n",
       " 0.6966,\n",
       " 0.6966,\n",
       " 0.6965,\n",
       " 0.69635,\n",
       " 0.6962,\n",
       " 0.69605,\n",
       " 0.6959,\n",
       " 0.69575,\n",
       " 0.6955,\n",
       " 0.69545,\n",
       " 0.6953,\n",
       " 0.69525,\n",
       " 0.6951,\n",
       " 0.695,\n",
       " 0.69495,\n",
       " 0.69495,\n",
       " 0.6948,\n",
       " 0.69475,\n",
       " 0.6946,\n",
       " 0.69435,\n",
       " 0.6942,\n",
       " 0.6941,\n",
       " 0.69415,\n",
       " 0.69415,\n",
       " 0.69405,\n",
       " 0.69405,\n",
       " 0.69395,\n",
       " 0.69375,\n",
       " 0.69375,\n",
       " 0.6937,\n",
       " 0.69375,\n",
       " 0.6938,\n",
       " 0.6937,\n",
       " 0.6936,\n",
       " 0.69355,\n",
       " 0.69355,\n",
       " 0.69355,\n",
       " 0.6932,\n",
       " 0.6931,\n",
       " 0.6932,\n",
       " 0.6933,\n",
       " 0.69305,\n",
       " 0.693,\n",
       " 0.69295,\n",
       " 0.69285,\n",
       " 0.6927,\n",
       " 0.69265,\n",
       " 0.69275,\n",
       " 0.6926,\n",
       " 0.69255,\n",
       " 0.6924,\n",
       " 0.69245,\n",
       " 0.69245,\n",
       " 0.6921,\n",
       " 0.69185,\n",
       " 0.69165,\n",
       " 0.6915,\n",
       " 0.69145,\n",
       " 0.69135,\n",
       " 0.69105,\n",
       " 0.6911,\n",
       " 0.6909,\n",
       " 0.6909,\n",
       " 0.6909,\n",
       " 0.69095,\n",
       " 0.6908,\n",
       " 0.6908]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
